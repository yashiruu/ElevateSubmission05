{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "77d238b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1feafbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://fashion-studio.dicoding.dev\"\n",
    "TOTAL_PAGES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c51f7e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_rating(text: str) -> float:\n",
    "    try:\n",
    "        return float(text)\n",
    "    except (ValueError, TypeError):\n",
    "        return math.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3073daf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_session() -> requests.Session:\n",
    "    \"\"\"\n",
    "    Membuat session requests agar lebih efisien\n",
    "    \"\"\"\n",
    "    session = requests.Session()\n",
    "    session.headers.update(\n",
    "        {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (compatible; ETL-Bot/1.0)\"\n",
    "        }\n",
    "    )\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7009b214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(session: requests.Session, page: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Mengambil data produk dari satu halaman website\n",
    "    \"\"\"\n",
    "    products = []\n",
    "    # url = f\"{BASE_URL}/page{page}\"\n",
    "    url = f\"{BASE_URL}/page{page}\" if page != 1 else BASE_URL\n",
    "\n",
    "    try:\n",
    "        response = session.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        # Error handling: gagal request halaman\n",
    "        print(f\"[ERROR] Failed to fetch page {page}: {e}\")\n",
    "        return products\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    cards = soup.find_all(\"div\", class_=\"product-details\")\n",
    "    # cards = [soup.find(\"div\", class_=\"product-details\")]\n",
    "    \n",
    "    for card in cards:\n",
    "        try:\n",
    "            data = [\n",
    "                el for el in card.find_all(recursive=False)\n",
    "            ]\n",
    "            \n",
    "            if len(data) != 6:\n",
    "                print(f\"[WARNING] Unexpected structure: {len(data)} elements\")\n",
    "                continue\n",
    "            \n",
    "            products.append(\n",
    "                {\n",
    "                    \"title\": data[0].get_text(strip=False),\n",
    "                    \"price\": data[1].get_text(strip=False),\n",
    "                    \"rating\": parse_rating(data[2].get_text(strip=False).split()[-3]),\n",
    "                    \"colors\": data[3].get_text(strip=False).split()[0],\n",
    "                    \"size\": data[4].get_text(strip=False).split()[1],\n",
    "                    \"gender\": data[5].get_text(strip=False).split()[1],\n",
    "                }\n",
    "            )            \n",
    "            # print(data, \"\\n\")\n",
    "\n",
    "        except AttributeError as e:\n",
    "            # Error handling: struktur HTML tidak sesuai\n",
    "            print(f\"[WARNING] Skipped one product due to parsing issue: {e}\")\n",
    "            continue\n",
    "\n",
    "    # debug\n",
    "    # print(json.dumps(products, indent=2, ensure_ascii=False))\n",
    "    return products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d1d5b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_all_products() -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Mengambil seluruh data produk dari halaman 1â€“50\n",
    "    \"\"\"\n",
    "    session = _create_session()\n",
    "    all_products: List[Dict] = []\n",
    "    timestamp = datetime.utcnow().isoformat()\n",
    "\n",
    "    for page in range(1, TOTAL_PAGES + 1):\n",
    "        page_products = scrape_page(session, page)\n",
    "\n",
    "        for product in page_products:\n",
    "            product[\"timestamp\"] = timestamp\n",
    "\n",
    "        all_products.extend(page_products)\n",
    "\n",
    "    return all_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2925c2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data: 1000\n",
      "{'title': 'Unknown Product', 'price': '$100.00', 'rating': nan, 'colors': '5', 'size': 'M', 'gender': 'Men', 'timestamp': '2026-02-10T04:42:45.110720'}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data = scrape_all_products()\n",
    "    print(f\"Total Data: {len(data)}\")\n",
    "    print(data[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
