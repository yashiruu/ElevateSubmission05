# Fashion Studio ETL Pipeline

## ğŸ“Œ Project Overview

This project implements a simple **ETL (Extract, Transform, Load) pipeline** to scrape competitor product data from: https://fashion-studio.dicoding.dev

The pipeline:

1. Extracts product data from 50 pages.
2. Transforms and cleans the dataset.
3. Loads the cleaned data into:
   - CSV (Flat File)
   - Google Sheets
   - PostgreSQL

## ğŸ— ETL Architecture

            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ fashion-studio.dicoding.dev â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Extract  â”‚
                    â”‚ (requests) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                           â”‚ Raw data (list[dict])
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Transform  â”‚
                    â”‚ (pandas)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                           â”‚ Clean DataFrame
                           â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â–¼               â–¼                 â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   CSV    â”‚    â”‚ Google Sheets â”‚   â”‚ PostgreSQL  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

## ğŸ§© Pipeline Flow

1. Extract all 50 pages
2. Clean & transform columns
3. Remove invalid, duplicate, and null data
4. Store cleaned dataset into three repositories

## â–¶ Run Pipeline

```
python3 main.py
```

## ğŸ—‚ Project Structure

```
ElevateSubmission05/
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ extract.py
â”‚ â”œâ”€â”€ transform.py
â”‚ â””â”€â”€ load.py
â”œâ”€â”€ tests/
â”‚ â”œâ”€â”€ test_extract.py
â”‚ â”œâ”€â”€ test_transform.py
â”‚ â””â”€â”€ test_load.py
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ submission.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Features

### âœ” Extract

- Scrapes product data (Title, Price, Rating, Colors, Size, Gender)
- Extracts from all 50 pages
- Adds extraction timestamp
- Handles network and parsing errors

### âœ” Transform

- Converts price from USD to IDR (1 USD = Rp16.000)
- Cleans rating into float
- Extracts integer from colors
- Removes prefix from size & gender
- Removes:
  - Null values
  - Duplicates
  - "Unknown Product"
- Ensures correct data types

### âœ” Load

Stores cleaned data into:

- CSV file
- Google Sheets
- PostgreSQL database

Each load function includes error handling.

## ğŸš€ Installation

Create virtual environment:

```bash
python3 -m venv .venv
source .venv/bin/activate
```

Install dependencies:

```bash
pip install -r requirements.txt
```

### ğŸ” Environment Variables

Create a .env file in the root directory:

```
DB_URI=postgresql+psycopg2://username:password@localhost:5432/<database_name>
GOOGLE_SPREADSHEET_ID=your_spreadsheet_id
```

### ğŸ“„ google-sheets-api.json

You must generate a Google Service Account key:

1. Go to Google Cloud Console.
2. Enable Google Sheets API.
3. Create a Service Account.
4. Generate a JSON Key.
5. Save it as:

```
google-sheets-api.json
```

Place it in the project root directory.

### â–¶ï¸ Running the ETL Pipeline

```
python3 main.py
```

### ğŸ§ª Running Unit Tests

```
python3 -m pytest tests
```

### ğŸ“Š Running Test Coverage

```
coverage run -m pytest tests
coverage report
```

Current coverage: â‰¥80%

## ğŸ“„ URL Google Sheets:

https://docs.google.com/spreadsheets/d/1tWZkZovHqIjOPCy7MkupIkFS7DC3f_IdBxCQ-QRHOWQ
